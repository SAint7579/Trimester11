{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "#Reading the text file\n",
    "dataset = []\n",
    "with open(\"TextFile.txt\") as file:\n",
    "    dataset = file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "break_removed = []\n",
    "for x in dataset:\n",
    "    break_removed.append(x[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome to Natural Language Processing',\n",
       " 'It is one of the most exciting research areas as of today',\n",
       " 'We will see how Python can be used to work with text files']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "break_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting everything to lower case\n",
    "lower = []\n",
    "for x in break_removed:\n",
    "    lower.append(x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['welcome to natural language processing',\n",
       " 'it is one of the most exciting research areas as of today',\n",
       " 'we will see how python can be used to work with text files']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['welcome', 'to', 'natural', 'language', 'processing'],\n",
       " ['it',\n",
       "  'is',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'most',\n",
       "  'exciting',\n",
       "  'research',\n",
       "  'areas',\n",
       "  'as',\n",
       "  'of',\n",
       "  'today'],\n",
       " ['we',\n",
       "  'will',\n",
       "  'see',\n",
       "  'how',\n",
       "  'python',\n",
       "  'can',\n",
       "  'be',\n",
       "  'used',\n",
       "  'to',\n",
       "  'work',\n",
       "  'with',\n",
       "  'text',\n",
       "  'files']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenizing the words\n",
    "tokenized = []\n",
    "for x in lower:\n",
    "    tokenized.append(nltk.word_tokenize(x))\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words Removal and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vishw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Removing Stop Words\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "stpwrds = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making stemmer object\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "obj = PorterStemmer()\n",
    "\n",
    "#Applying stemming and porting\n",
    "final_stemmed = []\n",
    "for x in tokenized:\n",
    "    processed = [obj.stem(word) for word in x if word not in stpwrds]\n",
    "    final_stemmed.append(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['welcom', 'natur', 'languag', 'process'],\n",
       " ['one', 'excit', 'research', 'area', 'today'],\n",
       " ['see', 'python', 'use', 'work', 'text', 'file']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tag = []\n",
    "\n",
    "for x in tokenized:\n",
    "    pos_tag.append(nltk.pos_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('welcome', 'NN'),\n",
       "  ('to', 'TO'),\n",
       "  ('natural', 'JJ'),\n",
       "  ('language', 'NN'),\n",
       "  ('processing', 'NN')],\n",
       " [('it', 'PRP'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('one', 'CD'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('most', 'RBS'),\n",
       "  ('exciting', 'JJ'),\n",
       "  ('research', 'NN'),\n",
       "  ('areas', 'NNS'),\n",
       "  ('as', 'IN'),\n",
       "  ('of', 'IN'),\n",
       "  ('today', 'NN')],\n",
       " [('we', 'PRP'),\n",
       "  ('will', 'MD'),\n",
       "  ('see', 'VB'),\n",
       "  ('how', 'WRB'),\n",
       "  ('python', 'NN'),\n",
       "  ('can', 'MD'),\n",
       "  ('be', 'VB'),\n",
       "  ('used', 'VBN'),\n",
       "  ('to', 'TO'),\n",
       "  ('work', 'VB'),\n",
       "  ('with', 'IN'),\n",
       "  ('text', 'JJ'),\n",
       "  ('files', 'NNS')]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcom natur languag process', 'one excit research area today', 'see python use work text file']\n",
      "[[0 0 0 1 1 0 1 0 0 0 0 0 0 1 0]\n",
      " [1 1 0 0 0 1 0 0 1 0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0 0 1 0 1 1 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=100)\n",
    "joined = []\n",
    "for x in final_stemmed:\n",
    "    joined.append(' '.join(x))\n",
    "print(joined)\n",
    "\n",
    "X = vectorizer.fit_transform(joined).toarray()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
